import uvicorn
from fastapi import FastAPI, BackgroundTasks, HTTPException
import mysql.connector
import requests
import xml.etree.ElementTree as ET
import re
from typing import List, Dict
import config

# --------------------------------------------------------------------------
# FastAPI ì•± ìƒì„±
# --------------------------------------------------------------------------
app = FastAPI(
    title="Worknet Job Scraper API",
    description="ê´€ì‹¬ ê¸°ì—…ì˜ ì±„ìš© ê³µê³ ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” APIì…ë‹ˆë‹¤.",
    version="1.0"
)

# --------------------------------------------------------------------------
# ì„¤ì • ë° ê¸°ì¡´ ë¡œì§ 
# --------------------------------------------------------------------------
db_config = {
    'host': 'localhost',
    'user': 'root',
    'password': 'dldudwns01~',
    'database': 'mysql'
}

# _normalize, build_interest_set, is_interest_company í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def _normalize(s: str) -> str:
    if not s: return ""
    s = re.sub(r"\(ì£¼\)", "", s)
    s = re.sub(r"[\sÂ·â€¢\-_/]", "", s)
    s = re.sub(r"[^\wê°€-í£]", "", s)
    return s.lower()

def build_interest_set(companies, aliases):
    norm_set = set()
    for c in companies: norm_set.add(_normalize(c))
    for a in aliases:
        if a: norm_set.add(_normalize(a))
    return norm_set

def is_interest_company(company_name: str, interest_norm_set) -> bool:
    n = _normalize(company_name)
    if not n: return False
    if n in interest_norm_set: return True
    for target in interest_norm_set:
        if target in n or n in target: return True
    return False

# --------------------------------------------------------------------------
# ë°ì´í„°ë² ì´ìŠ¤ ë° API í˜¸ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ì•½ê°„ ìˆ˜ì •)
# --------------------------------------------------------------------------

def fetch_companies_from_db() -> (List[str], List[str]):
    print("\nğŸ” DBì—ì„œ ê´€ì‹¬ ê¸°ì—… ë¶ˆëŸ¬ì˜¤ê¸°")
    conn = None
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("SELECT name, alias FROM companies")
        rows = cursor.fetchall()
        companies = [row[0] for row in rows if row[0]]
        aliases = [row[1] for row in rows if len(row) > 1 and row[1]]
        print(f"âœ… DB ê¸°ì—…ëª… {len(companies)}ê°œ, ë³„ì¹­ {len([a for a in aliases if a])}ê°œ ë¶ˆëŸ¬ì˜´")
        return companies, aliases
    except mysql.connector.Error as err:
        print(f"âŒ DB ì˜¤ë¥˜: {err}")
        raise HTTPException(status_code=500, detail=f"Database error: {err}")
    finally:
        if conn and conn.is_connected():
            conn.close()

def fetch_and_filter_jobs(companies, aliases) -> List[Dict]:
    # (ê¸°ì¡´ fetch_and_filter_jobs í•¨ìˆ˜ ë‚´ìš©ê³¼ ë™ì¼)
    if not companies: return []
    url = "https://www.work24.go.kr/cm/openApi/call/wk/callOpenApiSvcInfo210L21.do"
    all_raw_jobs = []
    interest_norm_set = build_interest_set(companies, aliases)
    print("\nğŸ“¦ 'ì±„ìš©ì •ë³´' API ìˆ˜ì§‘ ì‹œì‘\n")
    for page in range(1, 4):
        params = {"authKey": config.WORKNET_API_KEY, "callTp": "L", "returnType": "XML", "startPage": str(page), "display": "100"}
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            root = ET.fromstring(response.content)
            for job_item in root.findall(".//dhsOpenEmpInfo"):
                company_name = job_item.findtext("empBusiNm") or ""
                if not is_interest_company(company_name, interest_norm_set): continue
                print(f"ğŸ¯ ë§¤ì¹­ëœ ê¸°ì—…: {company_name}")
                job_data = {
                    "company_name": company_name, "job_title": job_item.findtext("empWantedTitle"),
                    "employment_type": job_item.findtext("empWantedTypeNm"), "start_date": job_item.findtext("empWantedStdt"),
                    "end_date": job_item.findtext("empWantedEndt"), "company_type": job_item.findtext("coClcdNm"),
                    "company_logo": job_item.findtext("regLogImgNm"), "apply_link": job_item.findtext("empWantedHomepgDetail"),
                    "job_id": job_item.findtext("wantedAuthNo")
                }
                all_raw_jobs.append(job_data)
        except Exception as e:
            print(f"âŒ API ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {e}")
            continue
    print(f"\nğŸ“Š ê´€ì‹¬ê¸°ì—… ê³µê³  ìˆ˜ì§‘ ê²°ê³¼: {len(all_raw_jobs)}ê±´")
    return all_raw_jobs


def upsert_jobs_to_db(data: List[Dict]):
    # (ê¸°ì¡´ upsert_jobs_to_db í•¨ìˆ˜ ë‚´ìš©ê³¼ ê±°ì˜ ë™ì¼, ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
    if not data:
        print("ğŸ’¡ ìˆ˜ì§‘ëœ ê³µê³ ê°€ ì—†ì–´ DBì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    conn = None
    try:
        # ì´ í•¨ìˆ˜ ë‚´ì—ì„œ ê¸°ì¡´ job_idë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¡œì§ì„ í¬í•¨
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("SELECT job_id FROM jobs")
        existing_job_ids = {row[0] for row in cursor.fetchall()}
        
        new_jobs = [job for job in data if job['job_id'] not in existing_job_ids]
        if not new_jobs:
            print("ğŸ’¡ ìƒˆë¡œìš´ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        job_tuples = [(job.get('company_name'), job.get('job_title'), job.get('employment_type'), job.get('start_date'),
                       job.get('end_date'), job.get('company_type'), job.get('company_logo'), job.get('apply_link'),
                       job.get('job_id')) for job in new_jobs]
        
        query = """
            INSERT INTO jobs (company_name, job_title, employment_type, start_date, end_date, company_type, company_logo, apply_link, job_id)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE company_name=VALUES(company_name), job_title=VALUES(job_title), employment_type=VALUES(employment_type), start_date=VALUES(start_date), end_date=VALUES(end_date), company_type=VALUES(company_type), company_logo=VALUES(company_logo), apply_link=VALUES(apply_link);
        """
        cursor.executemany(query, job_tuples)
        conn.commit()
        print(f"âœ… ì´ {cursor.rowcount}ê°œ ì‹ ê·œ ê³µê³ ê°€ DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except mysql.connector.Error as err:
        print(f"âŒ DB ì €ì¥ ì˜¤ë¥˜: {err}")
    finally:
        if conn and conn.is_connected():
            conn.close()


def create_jobs_table_if_not_exists():
    # (ê¸°ì¡´ create_jobs_table í•¨ìˆ˜ì™€ ë™ì¼)
    conn = None
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        create_table_query = """
            CREATE TABLE IF NOT EXISTS jobs (id INT AUTO_INCREMENT PRIMARY KEY, company_name VARCHAR(255), job_title VARCHAR(255), employment_type VARCHAR(255), start_date VARCHAR(255), end_date VARCHAR(255), company_type VARCHAR(255), company_logo VARCHAR(255), apply_link VARCHAR(255), job_id VARCHAR(255) UNIQUE, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
        """
        cursor.execute(create_table_query)
        print("âœ… 'jobs' í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆê±°ë‚˜ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
    except mysql.connector.Error as err:
        print(f"âŒ í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: {err}")
    finally:
        if conn and conn.is_connected():
            conn.close()

# --------------------------------------------------------------------------
# í•µì‹¬ ì‘ì—… ì‹¤í–‰ í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  ë¡œì§)
# --------------------------------------------------------------------------
def run_job_scraping_task():
    """ì±„ìš© ê³µê³  ìŠ¤í¬ë˜í•‘ë¶€í„° DB ì €ì¥ê¹Œì§€ì˜ ì „ì²´ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("\n=== [Background Task] Job Crawling ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ ===")
    create_jobs_table_if_not_exists()
    companies, aliases = fetch_companies_from_db()
    raw_jobs = fetch_and_filter_jobs(companies, aliases)
    
    if raw_jobs:
        upsert_jobs_to_db(raw_jobs)
        print("\nğŸ‰ [Background Task] ì±„ìš©ê³µê³  ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ")
    else:
        print("\nğŸ’¡ [Background Task] ìƒˆë¡œìš´ ê´€ì‹¬ ê¸°ì—… ê³µê³  ì—†ìŒ")
    print("\n=== [Background Task] Job Crawling ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¢…ë£Œ ===")


# --------------------------------------------------------------------------
# FastAPI ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# --------------------------------------------------------------------------
@app.post("/trigger-job-scrape", status_code=202)
def trigger_scrape(background_tasks: BackgroundTasks):
    """
    Worknet ì±„ìš© ê³µê³  ìŠ¤í¬ë˜í•‘ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print("ğŸš€ API ìš”ì²­ ìˆ˜ì‹ : ì±„ìš© ê³µê³  ìŠ¤í¬ë˜í•‘ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.")
    background_tasks.add_task(run_job_scraping_task)
    return {"message": "Job scraping task has been started in the background."}

# --------------------------------------------------------------------------
# ì„œë²„ ì‹¤í–‰ (í„°ë¯¸ë„ì—ì„œ uvicorn job_scraper_api:app --reload)
# --------------------------------------------------------------------------
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)